# Daily Newsletter - 2025-12-27

*Generated on 2025-12-28 07:19:26 UTC*

## Table of Contents

**Total items: 7**

- [Article](#article) (1 items)
- [Arxiv](#arxiv) (2 items)
- [Huggingface](#huggingface) (2 items)
- [Youtube](#youtube) (2 items)

---

## Arxiv

*2 item(s)*

### 1. [2512.12967] QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management

**Source:** [https://arxiv.org/html/2512.12967](https://arxiv.org/html/2512.12967)

## What did the author accomplish ?

- **What**  
  Introduced **QwenLongâ€‘L1.5**, a 30â€‘B parameter LLM that attains stateâ€‘ofâ€‘theâ€‘art longâ€‘context reasoning (up toâ€¯4â€¯M tokens) through a **complete postâ€‘training recipe**.  

- **Why**  
  Existing work focuses on preâ€‘training or architectural tricks, leaving a gap in **postâ€‘training methods** that can (i) generate highâ€‘quality longâ€‘context data, (ii) train stably on extremely long sequences, and (iii) enable reasoning beyond the physical context window. QwenLongâ€‘L1.5 fills this gap, delivering performance comparable to proprietary flagship models (GPTâ€‘5, Geminiâ€‘2.5â€‘Pro) while remaining fully openâ€‘source.


### Visual Summary  

![Overall results of QwenLongâ€‘L1.5 across six longâ€‘context benchmarks (Figureâ€¯1 from the paper)](https://arxiv.org/html/2512.12967v1/x1.png){width=600px}


## Training compute

The paper does **not disclose exact GPU hours or cluster size**. Training is performed on the 30â€¯B base model with onâ€‘policy RL, which typically requires highâ€‘memory GPUs (e.g., 8â€¯Ã—â€¯A100â€¯40â€¯GB or similar) to handle 256â€¯Kâ€‘token windows and the multiâ€‘stage schedule. Users should provision sufficient GPU memory to fit the model + KV cache for the longest stage (â‰ˆâ€¯120â€¯K tokens).

---

### 2. [2509.25123] From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones

**Source:** [https://arxiv.org/html/2509.25123](https://arxiv.org/html/2509.25123)

## What did the authors accomplish?  

- **What** â€“ Demonstrated that reinforcementâ€‘learning (RL) postâ€‘training can *teach* large language models (LLMs) genuinely new reasoning skills, not just reâ€‘weight existing ones.  
- **Why** â€“ This settles a heated debate: many recent works claim RL only â€œreranksâ€ the base modelâ€™s outputs, but the authors provide concrete, controlled evidence that RL can *compose* previously learned atomic abilities into higherâ€‘order capabilities that generalize to unseen compositions, deeper nesting, and even to a different task (Countdown).  


## What can you use yourself?  

| Resource / Tool | Description / Link |
|-----------------|--------------------|
| **Model** | Llamaâ€‘3.1â€‘8Bâ€‘Instruct (used as the base LLM). |
| **RL optimizer** | DAPO (Distributed Actorâ€‘Critic with Preference Optimization) â€“ used with GRPO rewards. |
| **Fineâ€‘tuning method** | Rejection Fineâ€‘Tuning (RFT) â€“ openâ€‘source implementations available in the `verL` repo. |
| **Dataset** | Synthetic stringâ€‘transformation suite (25 functions, 50â€¯k Levelâ€‘1 & Levelâ€‘2 examples). Code for all functions is in Appendixâ€¯D of the paper. |
| **Evaluation scripts** | Pass@k, Avg@32 for Countdown, and failureâ€‘mode classifier (based on Geminiâ€‘2.5â€‘Pro). |
| **Training hyperâ€‘parameters** (the ones that worked best) | â€¢ Stageâ€¯1 RFT: 2 epochs, lrâ€¯=â€¯2eâ€‘5, batchâ€¯=â€¯128. <br>â€¢ Stageâ€¯2 RL: DAPO, lrâ€¯=â€¯1eâ€‘6, onâ€‘policy batchâ€¯=â€¯16, rollout sizeâ€¯=â€¯16, KLâ€¯=â€¯0, entropyâ€¯=â€¯0. <br>â€¢ RFT baseline (iterative): same lr as Stageâ€¯1, 128â€‘batch, repeat until convergence. |
| **Methodology you can adopt** | 1ï¸âƒ£ Preâ€‘train (or start from a strong base LLM).  <br>2ï¸âƒ£ Use RFT on *atomic* tasks to embed lowâ€‘level skills.  <br>3ï¸âƒ£ Construct a *compositional* RL dataset where the reward only signals overall correctness.  <br>4ï¸âƒ£ Train with GRPO/DAPO â€“ the binary reward forces the model to discover the composition rule.  <br>5ï¸âƒ£ Validate on heldâ€‘out deeper compositions and on a downstream task that shares the same atomic primitives. |
| **Potential extensions** | â€¢ Replace synthetic strings with realâ€‘world functions (e.g., code APIs, math operators). <br>â€¢ Scale to larger models (13B, 70B) to test whether the same RL incentive yields bigger gains. <br>â€¢ Combine RL with curriculum learning: start from Levelâ€‘1, gradually increase nesting depth. |


## References to further follow / read  

- **Core RL papers** â€“ DAPO: *â€œDistributed Actorâ€‘Critic with Preference Optimizationâ€* (2024). <br>GRPO: *â€œGroup Relative Preference Optimizationâ€* (2024).  
- **Human skillâ€‘composition theory** â€“ Anderson, J. R. (1982). *Acquisition of Cognitive Skills*.  
- **Contrasting viewpoints** â€“ Yue etâ€¯al. (2025) *â€œDoes RL really improve LLMs?â€*; Gandhiâ€¯etâ€¯al. (2025) *â€œCognitive Behavior Transferâ€*.  
- **Related compositional work** â€“ Yinâ€¯etâ€¯al. (2025) *Learning Compositionality via Inâ€‘Context Learning*; Sunâ€¯etâ€¯al. (2025) *OMEGACL* (shows RL without explicit compositional incentive fails).  
- **Openâ€‘source code** â€“ The authors release the synthetic dataset, function definitions, and training scripts (GitHub link in the paperâ€™s supplementary material).

---

## Huggingface

*2 item(s)*

### 1. Paper page - GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training

**Source:** [https://arxiv.org/html/2512.13043](https://arxiv.org/html/2512.13043)

## What did the author accomplish ?

- **What**  
  Introduced **GTRâ€‘Turbo**, a lightweight upgrade to Guided Thought Reinforcement (GTR) that eliminates the need for an external, expensive teacher model when training multiâ€‘turn visionâ€‘language agents. By merging historical RL checkpoints into a â€œfreeâ€ teacher, GTRâ€‘Turbo provides dense, stepâ€‘level guidance via supervised fineâ€‘tuning (SFT) or KLâ€‘based logit distillation.

- **Why**  
  Multiâ€‘turn RL for VLM agents suffers from sparse rewards, longâ€‘horizon credit assignment, and â€œthought/entropy collapseâ€. Prior solutions (e.g., GTR, Onâ€‘Policy Distillation) rely on costly privileged models (GPTâ€‘4o, Gemini), limiting scalability, reproducibility, and increasing training time and monetary cost. GTRâ€‘Turbo offers a selfâ€‘contained, cheaper, and faster alternative while achieving equal or superior performance.


## What can you use yourself?

- **Tools & Resources**  
  - **Base model**: `Qwen2.5â€‘VLâ€‘7Bâ€‘Instruct` (and later `Qwen3â€‘VLâ€‘8Bâ€‘Instruct`).  
  - **Merging method**: **TIES** (Trimâ€‘Electâ€‘Sign) â€“ see Yadav etâ€¯al., 2023.  
  - **RL algorithm**: PPO (Schulman etâ€¯al., 2017) with standard hyperâ€‘parameters (clipâ€¯=â€¯0.1, entropyâ€¯=â€¯0.01, etc.).  
  - **LoRA** fineâ€‘tuning (Hu etâ€¯al., 2022) to keep GPU memory low.  

- **Recipes / Methodologies**  
  1. **Checkpoint Buffer & Merging** â€“ Store every PPO checkpoint; merge after each epoch using TIES (densityâ€¯=â€¯0.8).  
  2. **Thought Guidance** â€“ Choose between:  
     - **SFT**: Collect teacherâ€‘generated thoughts in a dataset `ğ’Ÿ` and add an SFT loss.  
     - **KLâ€‘Distillation**: Compute reverse KL on the fly and subtract it (scaled by Î²â€¯=â€¯1) from the environment reward.  
  3. **Weight Averaging** â€“ SMA works outâ€‘ofâ€‘theâ€‘box; EMA with Î±â€¯â‰ˆâ€¯0.5 gives a good tradeâ€‘off.  

- **Hyperâ€‘parameters / Best Practices** (taken from Appendixâ€¯B)

| Category | Parameter | Value |
|----------|-----------|-------|
| Learning rate schedule | CosineAnnealingLR | 1eâ€‘5 â†’ 1eâ€‘9 (25 steps) |
| Discount factor (Î³) | 0.9 |
| GAE Î» | 0.95 |
| PPO clip Îµ | 0.1 |
| Entropy coeff. | 0.01 |
| Value loss coeff. | 0.5 |
| LoRA rank (r) | 128 |
| LoRA Î± | 256 |
| LoRA dropout | 0.05 |
| KL loss coeff. (Î²) | 1 (KL variant) |
| Thought probability | 0.5 (Points24), 0.2 (ALFWorld) |
| TIES density | 0.8 |
| Generation temperature (agent) | 0.2 |
| Teacher temperature (SFT) | 0.2 â€“ 0.9 (ramp) |

- **Datasets**  
  - **Points24** (cardâ€‘game with arithmetic reasoning) â€“ labels from a task solver.  
  - **ALFWorld** (embodied household tasks) â€“ imageâ€‘only observations; subâ€‘goal/reward definitions as in the paper.  

- **Potential Integration**  
  - Apply GTRâ€‘Turbo to any VLMâ€‘based agent where a strong external teacher is unavailable (e.g., robotics, videoâ€‘game bots).  
  - Combine with other RL tricks (GRPO, DAPO) for further stability.  
  - Use the mergedâ€‘checkpoint teacher to generate synthetic â€œprocessâ€ data for downstream fineâ€‘tuning.


## References to further follow / read ?

- **Guided Thought Reinforcement (GTR)** â€“ Wei etâ€¯al., 2025.  
- **Onâ€‘Policy Distillation** â€“ Lu etâ€¯al., 2025.  
- **TIES merging** â€“ Yadav etâ€¯al., â€œTrimâ€‘Electâ€‘Sign for Model Mergingâ€, 2023.  
- **Model merging literature** â€“ Ilharco etâ€¯al., 2022; Yu etâ€¯al., 2024; Li etâ€¯al., 2025.  
- **RL4VLM framework** â€“ Zhai etâ€¯al., 2025.  
- **ALFWorld benchmark** â€“ Sridhar etâ€¯al., 2020.  
- **Points24 benchmark** â€“ Zhai etâ€¯al., 2025.  
- **LoRA** â€“ Hu etâ€¯al., 2022.  

**Paper & Code**: https://arxiv.org/html/2512.13043 (full PDF, figures, and pseudocode).  
**Model checkpoints** (Qwen2.5â€‘VLâ€‘7B, Qwen3â€‘VLâ€‘8B) are publicly available from the **Tencent AI Lab** model hub.

---

### 2. Paper page - Latent Implicit Visual Reasoning

**Source:** [https://arxiv.org/html/2512.21218](https://arxiv.org/html/2512.21218)

## What did the author accomplish ?

- **What** â€“ Introduced **Latent Implicit Visual Reasoning (LIVR)**, a taskâ€‘agnostic method that lets large multimodal models (LMMs) discover and exploit *visual reasoning tokens* without any explicit intermediate supervision.  
- **Why** â€“ Existing LMMs are textâ€‘centric and struggle on visionâ€‘heavy tasks; prior works that inject visual intermediates require costly, taskâ€‘specific annotations and embed human bias. LIVR removes these constraints, achieving stateâ€‘ofâ€‘theâ€‘art performance on a wide suite of perceptionâ€‘centric benchmarks while also improving multiâ€‘task instruction tuning.


## What can you use yourself?

| Resource | Description |
|---|---|
| **Models** | Qwen2.5â€‘VLâ€‘3Bâ€‘Instruct, Qwen3â€‘VLâ€‘4Bâ€‘Instruct, LLaVAâ€‘OneVisionâ€‘1.5â€‘4Bâ€‘Instruct (all openâ€‘source). |
| **Code / Implementation** | The authors fineâ€‘tune using **LoRA** (rankâ€¯=â€¯16, Î±â€¯=â€¯32, dropoutâ€¯=â€¯0.05) on the language backbone; vision encoder & projector stay frozen. Only the latentâ€‘token embeddings are unfrozen. |
| **Datasets** | Custom 1kâ€‘example VQAâ€‘style training sets derived from COCO, ArtBenchâ€‘10, SPairâ€‘71k, HPatches, FunKPoint, MID, NIGHTS, and PixMoâ€‘Count. Evaluation uses the BLINK benchmark (9 perceptionâ€‘heavy tasks). |
| **Hyperâ€‘parameters (default)** | <ul><li>Kâ€¯=â€¯16 latent tokens (placed *after* the prompt).</li><li>Stageâ€¯1â€¯=â€¯4 epochs, Stageâ€¯2â€¯=â€¯6 epochs (totalâ€¯10) for singleâ€‘task.</li><li>Learning rateâ€¯=â€¯1eâ€‘4 (AdamW), weight decayâ€¯=â€¯0.01.</li><li>Batch sizeâ€¯=â€¯1 per GPU, 8â€‘step gradient accumulation â†’ effective batchâ€¯=â€¯8.</li></ul> |
| **Best practices** | â€¢ Use the *bothâ€‘sides* bottleneck (answerâ€¯â†›â€¯image, promptâ€¯â†›â€¯image). <br>â€¢ Keep latent embeddings *unshared* (each token learns its own vector). <br>â€¢ Position latents *after* the textual prompt for better conditioning. |
| **Potential integrations** | â€¢ Plug LIVR into any LMM that follows the visionâ€‘encoderâ€¯â†’â€¯projectorâ€¯â†’â€¯LLM pipeline. <br>â€¢ Combine with instructionâ€‘tuning or chainâ€‘ofâ€‘thought prompting for richer multimodal reasoning. |


## References to further follow / read ?

- **Paper** â€“ *Latent Implicit Visual Reasoning* (arXivâ€¯2512.21218) â€“ https://arxiv.org/abs/2512.21218  
- **Related works**  
  - LLaVAâ€‘CoT, Visualâ€‘CoT, UVâ€‘CoT (textâ€‘based chainâ€‘ofâ€‘thought for vision).  
  - Aurora, Mirage (explicit visual intermediates).  
  - Coconut, â€œThink Before You Speakâ€ (latentâ€‘space reasoning).  
- **Datasets** (all publicly available)  
  - BLINK benchmark â€“ https://github.com/BLINK-benchmark  <br>
  - PixMoâ€‘Count â€“ https://huggingface.co/datasets/allenai/pixmo-count  <br>
  - COCO â€“ http://cocodataset.org  <br>
  - ArtBenchâ€‘10 â€“ https://github.com/ArtBench  <br>
  - SPairâ€‘71k â€“ https://github.com/zhenglinsp/SPair-71k  <br>
  - HPatches â€“ https://github.com/hpatches/hpatches-benchmark  <br>
  - FunKPoint â€“ https://github.com/zhenglinsp/FunKPoint  <br>
  - MID â€“ https://github.com/zhenglinsp/MID  <br>
  - NIGHTS (DreamSim) â€“ https://github.com/zhenglinsp/NIGHTS  

- **Implementation details** â€“ Appendixâ€¯B (training setup) and Appendixâ€¯A (dataset construction) in the paper.

---

## Youtube

*2 item(s)*

### 1. (2226) Steering LLM Behavior Without Fine-Tuning - YouTube

**Source:** [https://www.youtube.com/watch?v=F2jd5WuT-zg](https://www.youtube.com/watch?v=F2jd5WuT-zg)

## What did the author accomplish ?

- **What** â€“ Demonstrated a practical method to *steer* the behavior and â€œpersonalityâ€ of large language models (LLMs) **at inference time** without any fineâ€‘tuning or heavy promptâ€‘engineering.  
- **Why** â€“ To give researchers and developers a lightweight, reversible way to modulate model outputs (e.g., tone, factuality, ethical bias) while keeping the original weights untouched, opening new avenues for rapid prototyping, safetyâ€‘testing, and mechanistic interpretability.


## What can you use yourself?

| Resource | What it provides |
|----------|------------------|
| **Blog post / demo space** | Interactive notebook showing the full pipeline â€“ https://huggingface.co/spaces/dlouapre/eiffelâ€‘towerâ€‘llama |
| **Sparse Autoâ€‘Encoder collection** | Readyâ€‘made SAEs for several LLaMA variants â€“ https://huggingface.co/collections/dlouapre/sparse-auto-encoders-saes-for-mechanistic-interpretability |
| **Neuronpedia** | searchable database of discovered neurons & their semantics â€“ https://www.neuronpedia.org |
| **Python recipe** | Minimal 15â€‘line script (see snippet above) that can be dropped into any ğŸ¤—â€¯Transformers workflow. |
| **Bestâ€‘practice tips** | <ul><li>Use middleâ€‘range layers (8â€‘12) for best tradeâ€‘off between interpretability & impact.</li><li>Average hidden states over 10â€‘20 examples to reduce noise when computing steering vectors.</li><li>Scale the steering vector with a small coefficient (â‰ˆâ€¯0.1â€‘0.3) to avoid destabilising generation.</li></ul> |
| **Hyperâ€‘parameters** | â€¢ `layer = 12` (for 70â€‘B LLaMA)  <br>â€¢ `steering_scale = 0.2`  <br>â€¢ `batch_size = 4` for collecting example activations. |


## References to further follow / read ?

- **Main blog / demo** â€“ https://huggingface.co/spaces/dlouapre/eiffelâ€‘towerâ€‘llama  
- **Sparse Autoâ€‘Encoder hub** â€“ https://huggingface.co/collections/dlouapre/sparse-auto-encoders-saes-for-mechanistic-interpretability  
- **Neuronpedia** â€“ https://www.neuronpedia.org  
- **Related talks** (mechanistic interpretability & neuroâ€‘stimulation): <br>â€¢ *The most complex model we actually understand* â€“ Welch Labs (YouTube) <br>â€¢ *Transformers, the tech behind LLMs* â€“ 3Blue1Brown (YouTube) <br>â€¢ *RAG vs Fineâ€‘Tuning vs Prompt Engineering* â€“ IBM Technology (YouTube)

---

### 2. (2226) Designing a Customer-Centric Business Model - YouTube

**Source:** [https://www.youtube.com/watch?v=L1Km-hJt-uI](https://www.youtube.com/watch?v=L1Km-hJt-uI)

## What did the author accomplish ?

- **What** â€“ Michaelâ€¯Skok (founding partner at Underscoreâ€¯VC) delivered a concise, practical framework for **designing a customerâ€‘centric business model**. He showed how to surface the core value a startup creates and then shape the revenueâ€‘generation mechanics so that the company captures value **in concert with the customerâ€™s success**.  

- **Why** â€“ Most earlyâ€‘stage ventures build a product first and only later think about how the model will actually make money. By putting the **customerâ€™s outcomes at the centre of the model**, founders can create sustainable, defensible businesses that grow with their users instead of fighting against them.


## What can you use yourself?

| Resource / Tool | How to Apply |
|-----------------|--------------|
| **Startup Secrets Sandbox** â€“ interactive worksheets & templates | <https://bit.ly/3Cwv0nK> â€“ use the â€œCustomerâ€‘Value Canvasâ€ and â€œPricingâ€‘Fit Sheetâ€ to map your own value metric and pricing structure. |
| **Valueâ€‘Based Pricing Checklist** (from the talk) | 1. Identify measurable outcome 2. Quantify dollar impact 3. Set price as % of impact 4. Test with pilot 5. Refine. |
| **MVBM (Minimum Viable Business Model) Playbook** | Run a 2â€‘week pilot with a single customer segment, collect outcome data, and adjust pricing before full launch. |
| **Unitâ€‘Economics Calculator** (simple spreadsheet) | Plug in CAC, churn, LTV derived from the valueâ€‘based price to verify profitability early. |
| **Customerâ€‘Success Loop Template** | Set up a quarterly review cadence where product tweaks are directly tied to the value metric trends. |

*No specific hyperâ€‘parameters were discussed (the content is strategic, not technical).*


## References to further follow / read ?

- **Video:** â€œDesigning a Customerâ€‘Centric Business Modelâ€ â€“ Harvard Innovation Labs (Marâ€¯25â€¯2023) â€“ <https://www.youtube.com/watch?v=L1Km-hJt-uI>  
- **Speaker Bio:** Michaelâ€¯Skok, Underscoreâ€¯VC â€“ <https://underscoredvc.com/team/michael-skok/>  
- **Startup Secrets Sandbox (frameworks & templates):** <https://bit.ly/3Cwv0nK>  
- Related Harvard iâ€‘lab playlists (for deeper dives):  
  - *Vision, Mission & Culture* â€“ <https://www.youtube.com/watch?v=RI4UKUlnIDc>  
  - *Value Props: Create a Product People Will Actually Buy* â€“ <https://www.youtube.com/watch?v=q8d9uuO1Cf4>  
  - *Startup Business Models and Pricing | Startup School* â€“ <https://www.youtube.com/watch?v=oWZbWzAyHAE>  

These resources give you concrete tools to start building a business model that **captures value together with your customers**, turning their success into your sustainable revenue.

---

## Article

*1 item(s)*

### 1. The Optimal Token Baseline

**Source:** [https://yingru.notion.site/The-Optimal-Token-Baseline-399211a558b782cfa936014c0d42dfb8](https://yingru.notion.site/The-Optimal-Token-Baseline-399211a558b782cfa936014c0d42dfb8)

## What did the author accomplish ?

- **What**  
  The paper introduces the **Optimal Token Baseline (OTB)** â€“ a theoreticallyâ€‘derived, tokenâ€‘level varianceâ€‘reduction technique for onâ€‘policy reinforcement learning (RL) of large language models (LLMs). OTB eliminates the â€œtraining collapseâ€ that plagues longâ€‘horizon, sparseâ€‘reward RL by stabilising gradient norms.

- **Why**  
  In RLâ€‘fineâ€‘tuned LLMs, gradient variance grows with trajectory length and reward sparsity, causing sudden spikes in gradient norm and catastrophic performance drops. Existing baselines (groupâ€‘mean, leaveâ€‘oneâ€‘out, valueâ€‘function baselines) treat all tokens as equally noisy and therefore cannot control this variance, especially in multiâ€‘turn, toolâ€‘integrated reasoning (TIR) tasks.


## What can you use yourself?

- **Tools & Resources**  
  - **Code**: Integrated into the VeRL framework â€“ PRâ€¯[#4678](https://github.com/volcengine/verl/pull/4678)  
  - **Dataset**: Filtered rewardâ€‘augmented dataset â€“ HuggingFace ğŸ¤—â€¯[`Jiawei415/DPAO_filter`](https://huggingface.co/datasets/Jiawei415/DPAO_filter)  
  - **Models evaluated**:  
    - Qwen2.5â€‘7B (base)  
    - Qwen3â€‘8Bâ€‘Base & Qwen3â€‘14Bâ€‘Base  

- **Methodology / Recipes**  
  1. **Training loop** â€“ Full onâ€‘policy RL (REINFORCE style) with OTB as the baseline.  
  2. **Batch / Group size** â€“ Small groups (as low as **Nâ€¯=â€¯4**) achieve the same performance as Nâ€¯=â€¯32 baselines, cutting token usage by **â‰ˆâ€¯62â€¯% (singleâ€‘turn)** and **â‰ˆâ€¯66â€¯% (multiâ€‘turn TIR)**.  
  3. **Hyperâ€‘parameters (used in the paper)**  
     - Rollout batch size: **128** (TIR) / **64** (singleâ€‘turn)  
     - Miniâ€‘update size: **128**  
     - Max response length: **8192** (extended to 16â€¯k in ablations)  
     - Max turn (TIR): **5** (tested up to 10)  
     - Learning rate: **1eâ€‘6**  
     - Optimiser: AdamW (default VeRL settings)  

- **Practical Tips**  
  - Use the logitâ€‘gradient proxy to compute realised energy **without extra backward passes**.  
  - Exclude padding/EOS tokens when aggregating \(\hat W_t\); OTB naturally handles variableâ€‘length sequences.  
  - Monitor **gradient variance** (Appendixâ€¯D) â€“ a rising variance predicts imminent collapse; OTB keeps it flat.  
  - Combine OTB with **Geometricâ€‘Mean Sequence Masking** (Geoâ€‘RS) for larger models (14â€¯B) to further improve stability.


## References to further follow / read ?

1. **Original paper (preâ€‘print)** â€“ â€œThe Optimal Token Baseline: Variance Reduction for Longâ€‘Horizon LLMâ€‘RLâ€ â€“ Decâ€¯20â€¯2025.  
2. **Foundational varianceâ€‘reduction works**  
   - Dayan (1991) â€“ *Reinforcement Comparison*  
   - Greensmith etâ€¯al. (2004) â€“ *Variance reduction techniques for gradient estimates*  
   - Weaver & Tao (2013) â€“ *The optimal reward baseline for gradientâ€‘based RL*  
3. **Related LLMâ€‘RL baselines**  
   - GRPO (Groupâ€‘based REINFORCE) â€“ Shaoâ€¯etâ€¯al., 2024.  
   - RLOO â€“ Ahmadianâ€¯etâ€¯al., 2024.  
4. **Masking / Sampling strategies**  
   - Masked Importance Sampling (MIS) â€“ Liuâ€¯etâ€¯al., 2025.  
   - Geometricâ€‘Mean Sequence Masking (Geoâ€‘RS) â€“ Liuâ€¯etâ€¯al., 2025.  
5. **Logit dynamics** â€“ Liâ€¯Y., â€œLogit Dynamics in Softmax Policy Gradient Methodsâ€, arXivâ€¯2025.  
6. **Toolâ€‘Integrated Reasoning (TIR)** â€“ Xueâ€¯etâ€¯al., â€œSimpleTIRâ€, arXivâ€¯2025.

---

---

*End of newsletter for 2025-12-27*